<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>No-headache-hadoop by andreasjansson</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>No-headache-hadoop</h1>
        <p>Super simple hadoop setup</p>

        <p class="view"><a href="https://github.com/andreasjansson/no-headache-hadoop">View the Project on GitHub <small>andreasjansson/no-headache-hadoop</small></a></p>


        <ul>
          <li><a href="https://github.com/andreasjansson/no-headache-hadoop/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/andreasjansson/no-headache-hadoop/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/andreasjansson/no-headache-hadoop">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h3>
<a name="introduction" class="anchor" href="#introduction"><span class="octicon octicon-link"></span></a>Introduction</h3>

<p>This is a tutorial that describes a way to quickly setup a Hadoop cluster on cheap Amazon EC2 spot instances. We then show how to interact with the cluster by writing and running a few example Hadoop streaming jobs in Python.</p>

<h3>
<a name="initial-setup" class="anchor" href="#initial-setup"><span class="octicon octicon-link"></span></a>Initial setup</h3>

<p>For this tutorial I assume basic UNIX command line knowledge (if you know what a pipe is you'll be fine). All the code has been written and tested on Linux, but I'd imagine it would work on OSX and maybe even Cygwin.</p>

<p>The code I use is written in Python, so you'll need to make sure you have Python 2.7 <a href="https://wiki.python.org/moin/BeginnersGuide/Download">installed</a>. You'll also need <a href="http://blog.troygrosfield.com/2010/12/18/installing-easy_install-and-pip-for-python/">pip</a> (recommended) or easy_install to download Python packages.</p>

<p>Download <a href="https://github.com/andreasjansson/head-in-the-clouds">headintheclouds</a>:</p>

<pre><code>pip install headintheclouds
</code></pre>

<p>Download <a href="https://github.com/andreasjansson/no-headache-hadoop">no-headache-hadoop</a>:</p>

<pre><code>git clone https://github.com/andreasjansson/no-headache-hadoop.git
</code></pre>

<p>(If you don't have git installed, you can download the zip archive: <a href="https://github.com/andreasjansson/no-headache-hadoop/archive/master.zip">https://github.com/andreasjansson/no-headache-hadoop/archive/master.zip</a>)</p>

<p>Next, we need to create an AWS account. Go to <a href="https://aws.amazon.com/">https://aws.amazon.com/</a> and click Sign Up. You'll be asked for address, phone number, credit card details, etc. When you've finished signing up, go to <a href="https://console.aws.amazon.com/iam/home?#security_credential">https://console.aws.amazon.com/iam/home?#security_credential</a></p>

<p><img src="images/security_credential.png" alt="security credential screen"></p>

<p>Click Continue to Security Credentials. Under Access Keys, click Create New Access Key.</p>

<p><img src="images/new_access_key.png" alt="new access key"></p>

<p>Click Download Key File and store the rootkey.csv file somewhere secure.</p>

<p>Next, go to <a href="https://console.aws.amazon.com/">https://console.aws.amazon.com/</a> and click EC2. In the top right corner, change the region to US East (N. Virginia).</p>

<p><img src="images/us_east.png" alt="us east"></p>

<p>Under Network &amp; Security, click Key Pairs. Click Create Key Pair and give it a name. When you click Create you'll download a .pem key file, move that file somewhere secure. You'll also need to remove the write permissions on this file, e.g.</p>

<pre><code>chmod 400 andreas-ec2.pem
</code></pre>

<p>Finally, we need to set a few environment variables:</p>

<ul>
<li>Set <code>AWS_ACCESS_KEY_ID</code> to the AWSAccessKeyId from rootkey.csv</li>
<li>Set <code>AWS_SECRET_ACCESS_KEY</code> to the AWSSecretKey from rootkey.csv</li>
<li>Set <code>AWS_SSH_KEY_FILENAME</code> to the full path to the .pem key file, e.g. /home/andreas/secrets/andreas-ec2.pem</li>
<li>Set <code>AWS_KEYPAIR_NAME</code> to the name of the key pair, e.g. andreas-ec2</li>
</ul><p>You might want to place the exports in .bashrc, .profile, or equivalent:</p>

<pre><code>export AWS_ACCESS_KEY_ID=AKIJFKELJF98FNDNTHWQ
export AWS_SECRET_ACCESS_KEY=9FYZlblKV3sl/7QblWbVcSQeavN64+iiyvAoXoJI
export AWS_SSH_KEY_FILENAME=/home/andreas/secrets/andreas-ec2.pem
export AWS_KEYPAIR_NAME=andreas-ec2
</code></pre>

<p>To confirm that everything is working, <code>cd</code> to the no-headache-hadoop directory and type <code>fab debug_ec2</code> in the terminal.</p>

<h3>
<a name="launching-nodes" class="anchor" href="#launching-nodes"><span class="octicon octicon-link"></span></a>Launching nodes</h3>

<p>The real killer feature in AWS (at least for students and poorly funded academics) is their <a href="http://aws.amazon.com/ec2/spot-instances/">spot instance</a> offering. To run EC2, Amazon needs a lot of excess capacity. Instead of letting
all that hardware sit idle, they allow people to bid on unused instances. The hourly asking price is set based on supply and demand and changes frequently. If your bid exceeds the asking price, the instances you asked for will be launched. But if the asking price increases above your bid, all instances will be terminated immediately (and you will not be charged for the partial hour). Fortunately, Hadoop offers ways to deal with this.</p>

<p>To see the current spot pricing, type <code>fab pricing</code>. The output looks something like:</p>

<pre><code>AMAZON EC2:

size         compute_units  memory  recent  median  stddev  max    hourly_cost
t1.micro                 2     0.6   0.013   0.010   0.005  0.020        0.020
m1.small                 1     1.7   0.007   0.007   0.000  0.007        0.060
m1.medium                2    3.75   0.013   0.013   0.000  0.013        0.120
c1.medium                5     1.7   0.018   0.020   0.003  0.024        0.145
m1.large                 4     7.5   0.030   0.060   0.148  0.500        0.240
m2.xlarge              6.5    17.1   0.035   0.160   0.186  0.447        0.410
m1.xlarge                8      15   0.052   0.054   0.020  0.200        0.480
m3.xlarge               13      15   0.058   0.058   0.000  0.058        0.500
c1.xlarge               20       7   0.070   0.070   0.000  0.070        0.580
m2.2xlarge              13    34.2   0.070   0.070   0.000  0.070        0.820
m3.2xlarge              26      30   0.115   0.115   0.040  0.200        1.000
cc1.4xlarge           33.5      23   1.668   1.668   0.000  1.668        1.300
m2.4xlarge              26    68.4   0.400   0.280   0.545  1.800        1.640
cg1.4xlarge           33.5      22   2.100   2.100   0.775  2.100        2.100
cc2.8xlarge             88    60.5   0.270   0.270   0.000  0.270        2.400
cr1.8xlarge             88   244.0   0.343   0.343   0.083  0.510        3.500
</code></pre>

<p><code>recent</code> is the most recent spot price, <code>median</code>, <code>stddev</code> and <code>max</code> show the spot pricing over the past 24 hours. <code>hourly_cost</code> is the normal pay-as-you-go pricing. Looking at this table we see that spot instances are almost 10 times cheaper than normal instances.</p>

<p>To create new EC2 spot instances, type</p>

<pre><code>fab ec2.spot:ROLE,SIZE,PRICE,COUNT
</code></pre>

<p>where ROLE is a name we give the node, e.g. "slave"; SIZE is the instance size, e.g. "cc2.8xlarge"; PRICE is our bid, e.g. 0.28; COUNT is the number of nodes we want to launch (default 1).</p>

<p>To create normal ec2 instances, type</p>

<pre><code>fab ec2.create:ROLE,SIZE,COUNT
</code></pre>

<p>For this tutorial we'll create 10 worker nodes (slaves), one master, and one monitoring server. To keep costs down we'll make them all m1.medium instances with 3.75 GB memory and 2 <a href="http://aws.amazon.com/ec2/faqs/#What_is_an_EC2_Compute_Unit_and_why_did_you_introduce_it">compute units</a>. Currently, the price is $0.013/hour, and it's been constant for the past 24 hours, so it's probably safe to bid $0.015 (we'll only pay the asking price, even if our bid is higher).</p>

<pre><code>fab ec2.spot:master,m1.medium,0.015 &amp;
fab ec2.spot:monitoring,m1.medium,0.015 &amp;
fab ec2.spot:slave,m1.medium,0.015,10 &amp;
</code></pre>

<p>The ampersands at the end runs the command in the background so we create the instances in parallel.</p>

<p>Side note: For new users, Amazon limits the number of instances to 20. You probably want to increase that to at least 50, using the form at <a href="https://aws.amazon.com/contact-us/ec2-request/">https://aws.amazon.com/contact-us/ec2-request/</a></p>

<p>Once the requests have been fulfilled, we can type</p>

<p><code>fab nodes</code></p>

<p>which will list all instances managed by no-headache-hadoop, something like this:</p>

<pre><code>name        size       ip_address      private_dns_name                status      launch_time              
slave       m1.medium  23.20.2.145     ip-10-181-136-88.ec2.internal   running     2013-09-21 20:13:48+01:00
slave       m1.medium  50.17.24.121    ip-10-170-38-50.ec2.internal    running     2013-09-21 20:13:48+01:00
slave       m1.medium  107.22.106.11   ip-10-235-2-238.ec2.internal    running     2013-09-21 20:13:48+01:00
slave       m1.medium  54.211.124.93   ip-10-234-1-231.ec2.internal    running     2013-09-21 20:05:42+01:00
slave       m1.medium  54.221.146.1    ip-10-235-8-110.ec2.internal    running     2013-09-21 20:05:42+01:00
master      m1.medium  54.211.179.136  ip-10-181-163-28.ec2.internal   running     2013-09-21 20:05:42+01:00
slave       m1.medium  54.225.32.220   ip-10-28-108-156.ec2.internal   running     2013-09-21 20:13:48+01:00
slave       m1.medium  107.21.169.191  ip-10-181-160-147.ec2.internal  running     2013-09-21 20:13:48+01:00
monitoring  m1.medium  107.21.157.13   ip-10-232-69-149.ec2.internal   running     2013-09-21 20:05:42+01:00
slave       m1.medium  54.227.31.220   ip-10-232-56-202.ec2.internal   running     2013-09-21 20:05:42+01:00
slave       m1.medium  23.20.135.126   ip-10-235-32-78.ec2.internal    running     2013-09-21 20:13:48+01:00
slave       m1.medium  75.101.206.231  ip-10-182-148-58.ec2.internal   running     2013-09-21 20:13:48+01:00
</code></pre>

<p>If you go to the web console at <a href="https://console.aws.amazon.com/ec2">https://console.aws.amazon.com/ec2</a> you should see all instance names prefixed by "NHH-". (If not, something has gone wrong and you should terminate those instances through the web interface to avoid wasting money.)</p>

<p><img src="images/instances.png" alt="instances"></p>

<p>When we're done with our instances we <strong>must</strong> remember to terminate them using <code>fab terminate</code>.</p>

<p>In order to access the servers we need to open up the EC2 firewall on the SSH port (22). We do that by typing</p>

<pre><code>fab ec2.firewall:open=22
</code></pre>

<p>While we're at it, let's open some other ports we'll use later on:</p>

<pre><code>fab ec2.firewall:open=80
fab ec2.firewall:open=443
fab ec2.firewall:open=50030
fab ec2.firewall:open=50060
</code></pre>

<h3>
<a name="installing-the-software" class="anchor" href="#installing-the-software"><span class="octicon octicon-link"></span></a>Installing the software</h3>

<p>In the no-headache-hadoop repository, there is a folder called "puppet". It contains all the <a href="http://docs.puppetlabs.com/">puppet</a> <a href="http://docs.puppetlabs.com/learning/manifests.html#manifests">manifests</a> needed to install and configure Hadoop.</p>

<p>To install Hadoop on the servers we launched in the previous section, type</p>

<pre><code>fab build
</code></pre>

<p>This command will SSH into each of the servers in parallel, and install all the required software automatically. Expect this to take a few minutes and don't be surprised if some instances take much longer than others to finish installing. If it appears to hang, you can kill the script (Ctrl-C) and try again.</p>

<h3>
<a name="hadoop-overview" class="anchor" href="#hadoop-overview"><span class="octicon octicon-link"></span></a>Hadoop overview</h3>

<p>Before we move on to the example, it's probably good to look at what Hadoop actually is. This will just be a short overview, for a longer introduction to Hadoop, see <a href="http://developer.yahoo.com/hadoop/tutorial/">http://developer.yahoo.com/hadoop/tutorial/</a>.</p>

<p>At the core of Hadoop is the Hadoop Distributed File System (HDFS), a fault-tolerant, scalable file system optimised for few writes, many reads, and large files. Files are stored in 64MB blocks across multiple <em>data nodes</em>, with each block stored in several (3 by default) copies for redundancy. The <em>name node</em> keeps an index of which block belonging to which file is stored on which server.</p>

<p>The input to a Hadoop job is usually one or a few very large tab-separated files that are processed sequentially, line by line. The <em>job tracker</em> splits and distributes the input to many <em>task tracker</em> servers. The job tracker manages everything around the execution of jobs, e.g. taking failed task trackers out of the pool, combining results, etc.</p>

<p>A Hadoop job usually has two stages: map and reduce. The mapper transforms each input record to one (or sometimes more than one) intermediate record, in the format (key, output). These intermediates are then grouped by key, sorted, and fed to the reducer, that combines them into a single (or a few) outputs.</p>

<p>In our setup we run the name node and job tracker on the same master instance, and each slave runs both a data node and a task tracker. This is deliberate, the job tracker actually attempts to schedule tasks to the same machine that stores the data it will operate on.</p>

<p>Before we can start the cluster we need to format HDFS</p>

<pre><code>fab format
</code></pre>

<p>Now we can start the cluster using</p>

<pre><code>fab start
</code></pre>

<h2>
<a name="hadoop-streaming" class="anchor" href="#hadoop-streaming"><span class="octicon octicon-link"></span></a>Hadoop streaming</h2>

<p>Hadoop is written in Java, and you used to have to write map/reduce jobs in Java too. Fortunately, nowadays Hadoop has a tool called <a href="http://hadoop.apache.org/docs/stable/streaming.html">Hadoop streaming</a> that allows you to write Hadoop jobs in virtually any language. The streaming job reads input lines from stdin, processes the input, and writes the output to stdout.</p>

<p>A nice side effect of this is that we can test streaming jobs locally on the command line before we run them on the cluster:</p>

<pre><code>cat input | ./mapper | sort | ./reducer
</code></pre>

<h3>
<a name="monitoring-servers" class="anchor" href="#monitoring-servers"><span class="octicon octicon-link"></span></a>Monitoring servers</h3>

<p>The monitoring server we created earlier is running <a href="http://graphite.wikidot.com/">Graphite</a>, and the master and slave nodes all send it system stats using <a href="http://collectd.org/">collectd</a>.</p>

<p>The output of <code>fab nodes</code> gives us the external IP for each server. In the example above, the monitoring server is at 107.21.157.13. If enter that in a browser, we get something that looks like this:</p>

<p><img src="images/graphite.png" alt="graphite"></p>

<p>In the pane on the left we can navigate Graphite -&gt; collectd -&gt; [server private DNS name] -&gt; [metric group] -&gt; [metric name].</p>

<p><img src="images/graphite_metric.png" alt="graphite metric"></p>

<p>The most important metric to keep an eye on is memory -&gt; memory-used. Once that starts climbing towards 3GB, we are in trouble.</p>

<h3>
<a name="example-application" class="anchor" href="#example-application"><span class="octicon octicon-link"></span></a>Example application</h3>

<p>Now that we have our cluster built and configured, we can start doing some actual work on it. In this example we'll <a href="http://en.wikipedia.org/wiki/Association_rule_learning">mine frequent itemsets</a> of artists in the <a href="http://mtg.upf.edu/node/1671">lastfm360k</a> dataset, using a <a href="http://infolab.stanford.edu/%7Eechang/recsys08-69.pdf">parallel version of FP-growth</a>.</p>

<h2>
<a name="algorithm-overview" class="anchor" href="#algorithm-overview"><span class="octicon octicon-link"></span></a>Algorithm overview</h2>

<p>In frequent itemset mining we aim to find subsets of items that occur in many transations. The classic example is market basket analysis, where we want to find products that are frequently bought together. In this example we want to find sets of artists that occur in many people's Last.fm listening histories.</p>

<p>We will use a version of the <a href="http://en.wikipedia.org/wiki/Association_rule_learning#FP-growth_algorithm">FP-growth algorithm</a> that was designed as a set of map/reduce jobs.</p>

<p>First we split the dataset up into a number of "history" <em>shards</em> that will be processed in parallel. Then we count the number of occurrences of each artist. The individual artists are then grouped into artist shards. The history shards are processed in parallel, and a new data file is created containing (history artists, artist shard) tuples. Each history is output once for every artist shard in that history.</p>

<p>For example, if the artists are</p>

<pre><code>{A, B, C, D}
</code></pre>

<p>artist shards are</p>

<pre><code>{A =&gt; 1, B =&gt; 1, C =&gt; 2, D =&gt; 2}
</code></pre>

<p>and histories are</p>

<pre><code>{{A, B}, {A, B, C}, {B, D}, {C, D}}
</code></pre>

<p>The output will be</p>

<pre><code>({A, B}, 1)
({A, B, C}, 1)
({A, B, C}, 2)
({B, D}, 1)
({B, D}, 2)
({C, D}, 2)
</code></pre>

<p>The clever thing is that the history tuples can mined for frequent itemsets in parallel, since each artist shard has all of the transactions that artist appears in. The speedup is almost linear with the number of added servers.</p>

<p>The actual mining is done using traditional FP-growth. Finally, the results are deduplicated.</p>

<p>To download the code we will use in the example:</p>

<pre><code>git clone git@github.com:andreasjansson/parallel-frequent-itemset-mining-lastfm360k.git
</code></pre>

<p>The code consists of a number of Hadoop jobs that each have a mapper.py and a reducer.py.</p>

<h2>
<a name="downloading-the-dataset" class="anchor" href="#downloading-the-dataset"><span class="octicon octicon-link"></span></a>Downloading the dataset</h2>

<p>The dataset is over 500MB, so to save time and bandwidth we'll download it straight from Amazon. We do that by logging in to the master node:</p>

<pre><code>fab -R master ssh
</code></pre>

<p>The <code>-R</code> tells fabric to only apply the command to nodes with a certain role, in this case <code>master</code>.</p>

<p>To do anything Hadoop-related on the server, we need to change user to hadoop:</p>

<pre><code>sudo su - hadoop
</code></pre>

<p>Once we're logged in we download and unpack the data using</p>

<pre><code>wget http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz
tar xzvf lastfm-dataset-360K.tar.gz
</code></pre>

<p>The files are in tsv format so we can import them straight into HDFS. We will use usersha1-artmbid-artname-plays.tsv which has 1.7 million records, where a record consists of (user id, artist id, artist name, play count). If we look at the first few records we get an idea of what we're dealing with.</p>

<pre><code>head -n5 lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv
</code></pre>

<p>outputs</p>

<pre><code>00000c289a1829a808ac09c00daf10bc3c4e223b        3bd73256-3905-4f3a-97e2-8b341527f805    betty blowtorch 2137
00000c289a1829a808ac09c00daf10bc3c4e223b        f2fb0ff0-5679-42ec-a55c-15109ce6e320    die Ärzte       1099
00000c289a1829a808ac09c00daf10bc3c4e223b        b3ae82c2-e60b-4551-a76d-6620f1b456aa    melissa etheridge       897
00000c289a1829a808ac09c00daf10bc3c4e223b        3d6bbeb7-f90e-4d10-b440-e153c0d10b53    elvenking       717
00000c289a1829a808ac09c00daf10bc3c4e223b        bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8    juliette &amp; the licks    706
</code></pre>

<p>To import into HDFS, type</p>

<pre><code>hadoop dfs -copyFromLocal lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv /hadoop/lastfm-plays.tsv
</code></pre>

<p>This will copy the file to /hadoop/lastfm-plays.tsv in HDFS. We can browse HDFS using</p>

<pre><code>hadoop dfs -ls /
hadoop dfs -ls /hadoop
# etc.
</code></pre>

<h2>
<a name="preprocessing" class="anchor" href="#preprocessing"><span class="octicon octicon-link"></span></a>Preprocessing</h2>

<p>As a first step we transform the data so that each line is in the format (user id, list of artists). The <a href="https://github.com/andreasjansson/parallel-frequent-itemset-mining-lastfm360k/blob/master/preprocess/mapper.py">mapper</a> simply takes the input line and outputs (user id, artist name). The <a href="https://github.com/andreasjansson/parallel-frequent-itemset-mining-lastfm360k/blob/master/preprocess/reducer.py">reducer</a> then reads the input lines that have been sorted in Hadoop streaming by user id, and keeps the artists in memory. When the user id changes or we reach the end of the file, we output the old user id along with all the artists we've collected. Note that this would not work if the records hadn't been sorted before the reduce step.</p>

<p>If we have downloaded the data to our local computer as well as to the master node, we can test the job locally:</p>

<pre><code>head -n1000 /path/to/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv \
    | /path/to/pfp_lastfm360k/preprocess/mapper.py \
    | sort \
    | /path/to/pfp_lastfm360k/preprocess/reducer.py
</code></pre>

<h3>
<a name="where-to-go-from-here" class="anchor" href="#where-to-go-from-here"><span class="octicon octicon-link"></span></a>Where to go from here</h3>

<p>for no-headache-hadoop, install new software through puppet (especially python libs), etc.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/andreasjansson">andreasjansson</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>